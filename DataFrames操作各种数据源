import org.apache.spark.sql.SQLContext
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}
import scala.util.control._


object sparkSql {
  def main(args: Array[String]): Unit = {
    val conf = new SparkConf().setAppName("DataFrametest").setMaster("local")
      //setMaster("spark://192.168.137.31:7077")
      .set("spark.executor.memory", "400m")

    val sc = new SparkContext(conf)
    val sqlContext=new SQLContext(sc)

    //使用SQLContext，spark应用程序（Application）可以通过RDD、Hive表、JSON格式数据等数据源创建DataFrames
    //读取json文件,创建DataFrames
    val people = sqlContext.read.json("D:\\donghuang\\sparkDev\\src\\main\\resource\\people.json")
    people.show()
    people.printSchema()
    people.select("name").show()
    people.select(people.col("name"), people.col("age").plus(1)).show
    people.registerTempTable("people")
    //使用SQLContext的sql()方法执行SQL查询操作，sql()方法返回的查询结果为DataFrame格式
    sqlContext.sql("select id,name,age from people a where a.age>10").collect().foreach(println)
    println("===============================================================")

   //读取hive中的表
    val hiveContext = new HiveContext(sc)
    hiveContext.table("donghuang.khxx").registerTempTable("khxx")
    val cx = hiveContext.sql("select id,name,sjhm,hj,gxsj from khxx")
    cx.foreach(println)
    println("=====================================================")


    //jdbc 操作mysql
    val url="jdbc:mysql://cdh1:3306/hive"
    val prop = new java.util.Properties
    prop.setProperty("user","root")
    prop.setProperty("password","root1234")
    //#指定读取条件,这里 Array("TBL_ID>='80'") 是where过滤条件,可选参数
    val mysqltbls=sqlContext.read.jdbc(url,"TBLS",Array("TBL_ID>='80'"),prop)
    //DataFrames转换为RDD
    mysqltbls.rdd.map(a=>(a(0),1)).reduceByKey(_+_).foreach(println)
    mysqltbls.registerTempTable("mysqltbls")
    mysqltbls.show()
    //存储到文件,会生成有很多分片文件,如果mytest文件夹存在会报错
    //mysqltbls.rdd.saveAsTextFile("D:\\donghuang\\test\\src\\main\\resources\\mytest")
    //存储到mysql表里
    //mysqltbls.write.jdbc(url,"mytest",prop) //mytest表如果存在会报错
    //Accepted modes are 'overwrite', 'append', 'ignore', 'error'.
    mysqltbls.write.mode("append").jdbc(url,"mytest",prop)

    sqlContext.sql("select * from people left join mysqltbls").show() //如果操作hiveContext的khxx,会报表不存在错误
    sc.stop()

  }
}
